{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbaf51c-d356-4b9c-af88-2824ddbb5840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41827/1403109012.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686cf776-eb41-4f2f-86a3-5c26b663ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"../dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a28885c-08a7-4bfe-be56-2055c6f20f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea00edd-1cca-4eae-ad33-40d81f0d3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BasicTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.Series,\n",
    "        labels: List[int],\n",
    "        tokenizer: BasicTokenizer,\n",
    "        max_seq_length: int = 512,\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.labels = torch.tensor(labels, dtype= torch.float)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.input_ids, self.attention_mask = self._get_features(self.data)\n",
    "        \n",
    "    def _get_features(self, data: pd.Series) -> Tuple[torch.Tensor]:\n",
    "        features = self.tokenizer(\n",
    "            data.tolist(),\n",
    "            max_length=self.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return features[\"input_ids\"], features[\"attention_mask\"]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[index] ,\n",
    "            \"attention_mask\": self.attention_mask[index],\n",
    "            \"labels\": self.labels[index],\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba361a51-457f-48ae-a655-f8c8d6fc449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from typing import Any, Union, List, Callable, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BasicTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "    \n",
    "\n",
    "class ToxicCommentDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        dataset_path: str,\n",
    "        test_path: str,\n",
    "        max_seq_length: int = 512,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        test_size: float = 0.2,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.dataset = pd.read_parquet(dataset_path, columns=[\"comment_text\", \"labels\", \"label_w\"])\n",
    "        self.testset = pd.read_parquet(test_path, columns=[\"comment_text\", \"labels\", \"label_w\"])\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "    def create_sampler(self, data: pd.Series) -> WeightedRandomSampler:\n",
    "        data = data.to_numpy()\n",
    "        return WeightedRandomSampler(\n",
    "            weights=torch.from_numpy(data).type(torch.double),\n",
    "            num_samples=len(data),\n",
    "        )\n",
    "\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        train_df, val_df = train_test_split(self.dataset, test_size=self.test_size)\n",
    "        train_labels = train_df[\"labels\"].tolist()\n",
    "        val_labels = val_df[\"labels\"].tolist()\n",
    "        test_labels = self.testset[\"labels\"].tolist()\n",
    "        self.train_sampler = self.create_sampler(train_df[\"label_w\"])\n",
    "        self.train_dataset = MultiLabelDataset(\n",
    "            train_df[\"comment_text\"],\n",
    "            train_labels,\n",
    "            self.tokenizer,\n",
    "            self.max_seq_length,\n",
    "        )\n",
    "        self.val_dataset = MultiLabelDataset(\n",
    "            val_df[\"comment_text\"],\n",
    "            val_labels,\n",
    "            self.tokenizer,\n",
    "            self.max_seq_length,\n",
    "        )\n",
    "        self.test_dataset = MultiLabelDataset(\n",
    "            self.testset[\"comment_text\"],\n",
    "            test_labels,\n",
    "            self.tokenizer,\n",
    "            self.max_seq_length,\n",
    "        )\n",
    "         \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.eval_batch_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.train_batch_size, sampler=self.train_sampler)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfe7add-6d42-4d59-bad5-f39109035eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "class ToxicCommentClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        label_classes: List[str],\n",
    "        steps_per_epoch: Optional[int] = None,\n",
    "        n_epochs: int = 3,\n",
    "        lr: float = 1e-3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.id2label = {idx: label for idx, label in enumerate(label_classes)}\n",
    "        self.label2id = {label: idx for idx, label in enumerate(label_classes)}\n",
    "        self.num_labels = len(label_classes)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            num_labels=self.num_labels,\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            problem_type=\"multi_label_classification\",\n",
    "        )\n",
    "        self.f1 = torchmetrics.F1Score(task=\"multilabel\", num_labels=self.num_labels)\n",
    "        self.t_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=self.num_labels)\n",
    "        self.t_f1 = torchmetrics.F1Score(task=\"multilabel\", num_labels=self.num_labels)\n",
    "        self.v_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=self.num_labels)\n",
    "        self.v_f1 = torchmetrics.F1Score(task=\"multilabel\", num_labels=self.num_labels)\n",
    "        \n",
    "    def forward(self, **inputs: Any) -> Any:\n",
    "        return self.model(**inputs)\n",
    "    \n",
    "    def training_step(self, batch: Tuple[List[int], List[int]], batch_idx: int) -> float:\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        preds = torch.sigmoid(outputs.logits)\n",
    "        labels = batch[\"labels\"]\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.f1(preds, labels)\n",
    "        self.log(\"train_f1\", self.f1, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch: Tuple[List[int], List[int]], batch_idx: int) -> float:\n",
    "        outputs = self(**batch)\n",
    "        loss, logits = outputs[:2]\n",
    "        preds = torch.sigmoid(logits)\n",
    "        labels = batch[\"labels\"]\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.t_acc.update(preds, labels)\n",
    "        self.t_f1.update(preds, labels)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"labels\": labels}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.log(\"test_acc\", self.t_acc.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1\", self.t_f1.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.t_acc.reset()\n",
    "        self.t_f1.reset()\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[List[int], List[int]], batch_idx: int) -> float:\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "        preds = torch.sigmoid(logits)\n",
    "        labels = batch[\"labels\"]\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.v_acc.update(preds, labels)\n",
    "        self.v_f1.update(preds, labels)\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log(\"valid_acc\", self.v_acc.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_f1\", self.v_f1.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.v_acc.reset()\n",
    "        self.v_f1.reset()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters , lr=self.lr)\n",
    "        warmup_steps = self.steps_per_epoch//3\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6dcfb70-ed06-43a1-83af-b0cea00a67bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41827/3167931083.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  self.labels = torch.tensor(labels, dtype= torch.float)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"roberta-base\"\n",
    "pl_dataset = ToxicCommentDataModule(\n",
    "    model_name_or_path=model_name,\n",
    "    dataset_path=str(Path(DATASET_DIR) / \"train.parquet\"),\n",
    "    test_path=str(Path(DATASET_DIR) / \"test.parquet\"),\n",
    "    max_seq_length=512,\n",
    "    train_batch_size=10,\n",
    "    eval_batch_size=8,\n",
    "    test_size=0.2,\n",
    ")\n",
    "pl_dataset.setup(\"fit\")\n",
    "\n",
    "model_path = \"../checkpoints/lightning_logs/model_fit.ckpt\"\n",
    "if Path(model_path).exists():\n",
    "    pl_model = ToxicCommentClassifier.load_from_checkpoint(model_path)\n",
    "else:\n",
    "    pl_model = ToxicCommentClassifier(\n",
    "        model_name_or_path=model_name,\n",
    "        label_classes=labels,\n",
    "        steps_per_epoch=3000 // 12\n",
    "    )\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be099cdc-5ab8-4bb7-8fa1-44f961bc4c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ntub/Documents/190k-medium-articles/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 124 M \n",
      "1 | f1    | MultilabelF1Score                | 0     \n",
      "2 | t_acc | MultilabelAccuracy               | 0     \n",
      "3 | t_f1  | MultilabelF1Score                | 0     \n",
      "4 | v_acc | MultilabelAccuracy               | 0     \n",
      "5 | v_f1  | MultilabelF1Score                | 0     \n",
      "-----------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntub/Documents/190k-medium-articles/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ntub/Documents/190k-medium-articles/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564d6be8e4b449bea2271242d71603d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "acc_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filename=\"aac_{epoch}-{val_loss:.2f}-{valid_acc:.2f}\",\n",
    "    save_top_k=5,\n",
    ")\n",
    "loss_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"valid_f1\",\n",
    "    filename=\"loss_{epoch}-{val_loss:.2f}-{valid_f1:.2f}\",\n",
    "    save_top_k=5,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    limit_train_batches=4000,\n",
    "    limit_test_batches=2000,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    default_root_dir=\"../checkpoints\",\n",
    "    callbacks=[early_stop_callback, loss_checkpoint_callback, acc_checkpoint_callback],\n",
    "    reload_dataloaders_every_n_epochs=5,\n",
    ")\n",
    "trainer.fit(\n",
    "    pl_model,\n",
    "    datamodule=pl_dataset,\n",
    ")\n",
    "trainer.save_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b49b91d-f5c1-4f35-8d69-b24e543de1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe9ed245-a14b-4f33-939a-03b42fbdfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"comment_text\", \"labels\"]\n",
    "text_df = pd.read_parquet(\n",
    "    DATASET_DIR / \"train.parquet\",\n",
    "    columns=[\"comment_text\", \"labels\"],\n",
    ")\n",
    "device = torch.device(\"cuda:0\" if not torch.cuda.is_available() else \"cpu\")\n",
    "t_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "t_text = text_df.iloc[1003][\"comment_text\"]\n",
    "t_features = t_tokenizer(\n",
    "    [t_text],\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "t_inputs = {\n",
    "    \"input_ids\": t_features[\"input_ids\"].to(device) ,\n",
    "    \"attention_mask\": t_features[\"attention_mask\"].to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de58d902-420c-4d88-928c-3a6520d13ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pl_model(**t_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69e241d2-e765-4299-9595-6849a0156e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(text_df.iloc[1003][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1ce4aa3-61f8-4073-b83c-46e560400acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = outputs.logits.squeeze().sigmoid().detach().cpu().numpy()\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6889c784-9ab9-4556-ac67-4dd8b3bfab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [pl_model.id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba388189-fbcb-4f82-b94a-1f07613fbae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fedb10a7-2756-419b-8f6b-2fefe8a327a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfbd62-33ff-4f7e-ad9f-6df3bc0107b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
